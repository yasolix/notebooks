{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from codementor\n",
    "\n",
    "https://www.codementor.io/spark/tutorial/spark-python-data-aggregations\n",
    "\n",
    "##Getting the data and creating the RDD\n",
    "\n",
    "In this section we will use the reduced dataset (10 percent) provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a Gzip file that we will download locally. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MASTER=\"spark://127.0.0.1:7077\" SPARK_EXECUTOR_MEMORY=\"6G\" IPYTHON_OPTS=\"notebook --pylab inline\" ~/spark-1.3.1-bin-hadoop2.6/bin/pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import urllib\n",
    "f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 cloud-user hdfs   18115902 2017-01-17 03:35 /tmp/kddcup.data.gz\r\n",
      "-rw-r--r--   3 cloud-user hdfs    2144903 2017-01-17 08:52 /tmp/kddcup.data_10_percent.gz\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/kdd*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/tmp/kddcup.data_10_percent.gz': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put ./kddcup.data_10_percent.gz /tmp/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = \"/tmp/kddcup.data_10_percent.gz\"\n",
    "\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Inspecting interaction duration by tag\n",
    "\n",
    "Both fold and reduce take a function as an argument that is applied to two elements of the RDD. The fold action differs from reduce in that it gets and additional initial zero value to be used for the initial call. This value should be the identity element for the function provided.\n",
    "\n",
    "As an example, imagine we want to know the total duration of our interactions for normal and attack interactions. We can use reduce as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse data\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# separate into different RDDs\n",
    "normal_csv_data = csv_data.filter(lambda x: x[41]==\"normal.\")\n",
    "attack_csv_data = csv_data.filter(lambda x: x[41]!=\"normal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_duration_data = normal_csv_data.map(lambda x: int(x[0]))\n",
    "attack_duration_data = attack_csv_data.map(lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(normal_duration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration for 'normal' interactions is 21075991\n",
      "Total duration for 'attack' interactions is 2626792\n"
     ]
    }
   ],
   "source": [
    "total_normal_duration = normal_duration_data.reduce(lambda x, y: x + y)\n",
    "total_attack_duration = attack_duration_data.reduce(lambda x, y: x + y)\n",
    "\n",
    "print \"Total duration for 'normal' interactions is {}\".\\\n",
    "    format(total_normal_duration)\n",
    "print \"Total duration for 'attack' interactions is {}\".\\\n",
    "    format(total_attack_duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean duration for 'normal' interactions is 216.657\n",
      "Mean duration for 'attack' interactions is 6.621\n"
     ]
    }
   ],
   "source": [
    "normal_count = normal_duration_data.count()\n",
    "attack_count = attack_duration_data.count()\n",
    "\n",
    "print \"Mean duration for 'normal' interactions is {}\".\\\n",
    "    format(round(total_normal_duration/float(normal_count),3))\n",
    "print \"Mean duration for 'attack' interactions is {}\".\\\n",
    "    format(round(total_attack_duration/float(attack_count),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A better way, using aggregate\n",
    "\n",
    "The aggregate action frees us from the constraint of having the return be the same type as the RDD we are working on. Like with fold, we supply an initial zero value of the type we want to return. Then we provide two functions. The first one is used to combine the elements from our RDD with the accumulator. The second function is needed to merge two accumulators. Let's see it in action calculating the mean we did before. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_sum_count = normal_duration_data.aggregate(\n",
    "    (0,0), # the initial value\n",
    "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine val/acc\n",
    "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean duration for 'normal' interactions is 216.657\n"
     ]
    }
   ],
   "source": [
    "print \"Mean duration for 'normal' interactions is {}\".\\\n",
    "    format(round(normal_sum_count[0]/float(normal_sum_count[1]),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21075991, 97278)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_sum_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean duration for 'attack' interactions is 6.621\n"
     ]
    }
   ],
   "source": [
    "attack_sum_count = attack_duration_data.aggregate(\n",
    "    (0,0), # the initial value\n",
    "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n",
    "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "print \"Mean duration for 'attack' interactions is {}\".\\\n",
    "    format(round(attack_sum_count[0]/float(attack_sum_count[1]),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with key/value pair RDDs\n",
    "\n",
    "Spark provides specific functions to deal with RDDs which elements are key/value pairs. They are sually used to perform aggregations and other processings by key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "key_value_data = csv_data.map(lambda x: (x[41], x)) # x[41] contains the network interaction tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'normal.',\n",
       "  [u'0',\n",
       "   u'tcp',\n",
       "   u'http',\n",
       "   u'SF',\n",
       "   u'181',\n",
       "   u'5450',\n",
       "   u'0',\n",
       "   u'0',\n",
       "   u'0',\n",
       "   u'0',\n",
       "   u'0',\n",
       "   u'1',\n",
       "   u'0',\n",
       "   u'0',\n",
       "   u'0',\n",
       "   u'0',\n",
       "   u'0',\n",
       "   u'0',\n",
       "   u'0',\n",
       "   u'0',\n",
       "   u'0',\n",
       "   u'0',\n",
       "   u'8',\n",
       "   u'8',\n",
       "   u'0.00',\n",
       "   u'0.00',\n",
       "   u'0.00',\n",
       "   u'0.00',\n",
       "   u'1.00',\n",
       "   u'0.00',\n",
       "   u'0.00',\n",
       "   u'9',\n",
       "   u'9',\n",
       "   u'1.00',\n",
       "   u'0.00',\n",
       "   u'0.11',\n",
       "   u'0.00',\n",
       "   u'0.00',\n",
       "   u'0.00',\n",
       "   u'0.00',\n",
       "   u'0.00',\n",
       "   u'normal.'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'guess_passwd.', 144.0),\n",
       " (u'nmap.', 0.0),\n",
       " (u'warezmaster.', 301.0),\n",
       " (u'rootkit.', 1008.0),\n",
       " (u'warezclient.', 627563.0),\n",
       " (u'smurf.', 0.0),\n",
       " (u'pod.', 0.0),\n",
       " (u'neptune.', 0.0),\n",
       " (u'normal.', 21075991.0),\n",
       " (u'spy.', 636.0),\n",
       " (u'ftp_write.', 259.0),\n",
       " (u'phf.', 18.0),\n",
       " (u'portsweep.', 1991911.0),\n",
       " (u'teardrop.', 0.0),\n",
       " (u'buffer_overflow.', 2751.0),\n",
       " (u'land.', 0.0),\n",
       " (u'imap.', 72.0),\n",
       " (u'loadmodule.', 326.0),\n",
       " (u'perl.', 124.0),\n",
       " (u'multihop.', 1288.0),\n",
       " (u'back.', 284.0),\n",
       " (u'ipsweep.', 43.0),\n",
       " (u'satan.', 64.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_duration = csv_data.map(lambda x: (x[41], float(x[0]))) \n",
    "durations_by_key = key_value_duration.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "durations_by_key.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {u'back.': 2203,\n",
       "             u'buffer_overflow.': 30,\n",
       "             u'ftp_write.': 8,\n",
       "             u'guess_passwd.': 53,\n",
       "             u'imap.': 12,\n",
       "             u'ipsweep.': 1247,\n",
       "             u'land.': 21,\n",
       "             u'loadmodule.': 9,\n",
       "             u'multihop.': 7,\n",
       "             u'neptune.': 107201,\n",
       "             u'nmap.': 231,\n",
       "             u'normal.': 97278,\n",
       "             u'perl.': 3,\n",
       "             u'phf.': 4,\n",
       "             u'pod.': 264,\n",
       "             u'portsweep.': 1040,\n",
       "             u'rootkit.': 10,\n",
       "             u'satan.': 1589,\n",
       "             u'smurf.': 280790,\n",
       "             u'spy.': 2,\n",
       "             u'teardrop.': 979,\n",
       "             u'warezclient.': 1020,\n",
       "             u'warezmaster.': 20})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_by_key = key_value_data.countByKey()\n",
    "counts_by_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using combineByKey\n",
    "\n",
    "This is the most general of the per-key aggregation functions. Most of the other per-key combiners are implemented using it. We can think about it as the aggregate equivlent since it allows the user to return values that are not the same type as our input data.\n",
    "\n",
    "For example, we can use it to calculate per-type average durations as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate and collect in 0.042 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'back.': (284.0, 2203),\n",
       " u'buffer_overflow.': (2751.0, 30),\n",
       " u'ftp_write.': (259.0, 8),\n",
       " u'guess_passwd.': (144.0, 53),\n",
       " u'imap.': (72.0, 12),\n",
       " u'ipsweep.': (43.0, 1247),\n",
       " u'land.': (0.0, 21),\n",
       " u'loadmodule.': (326.0, 9),\n",
       " u'multihop.': (1288.0, 7),\n",
       " u'neptune.': (0.0, 107201),\n",
       " u'nmap.': (0.0, 231),\n",
       " u'normal.': (21075991.0, 97278),\n",
       " u'perl.': (124.0, 3),\n",
       " u'phf.': (18.0, 4),\n",
       " u'pod.': (0.0, 264),\n",
       " u'portsweep.': (1991911.0, 1040),\n",
       " u'rootkit.': (1008.0, 10),\n",
       " u'satan.': (64.0, 1589),\n",
       " u'smurf.': (0.0, 280790),\n",
       " u'spy.': (636.0, 2),\n",
       " u'teardrop.': (0.0, 979),\n",
       " u'warezclient.': (627563.0, 1020),\n",
       " u'warezmaster.': (301.0, 20)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "sum_counts = key_value_duration.combineByKey(\n",
    "    (lambda x: (x, 1)), # the initial value, with value x and count 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "tt= time() - t0\n",
    "print \"Aggregate  in {} secs\".format(round(tt,3))\n",
    "\n",
    "sum_counts.collectAsMap()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portsweep. 1915.299\n",
      "warezclient. 615.258\n",
      "spy. 318.0\n",
      "normal. 216.657\n",
      "multihop. 184.0\n",
      "rootkit. 100.8\n",
      "buffer_overflow. 91.7\n",
      "perl. 41.333\n",
      "loadmodule. 36.222\n",
      "ftp_write. 32.375\n",
      "warezmaster. 15.05\n",
      "imap. 6.0\n",
      "phf. 4.5\n",
      "guess_passwd. 2.717\n",
      "back. 0.129\n",
      "satan. 0.04\n",
      "ipsweep. 0.034\n",
      "nmap. 0.0\n",
      "smurf. 0.0\n",
      "pod. 0.0\n",
      "neptune. 0.0\n",
      "teardrop. 0.0\n",
      "land. 0.0\n"
     ]
    }
   ],
   "source": [
    "duration_means_by_type = sum_counts.map(lambda (key,value): (key, round(value[0]/value[1],3))).collectAsMap()\n",
    "\n",
    "# Print them sorted\n",
    "for tag in sorted(duration_means_by_type, key=duration_means_by_type.get, reverse=True):\n",
    "    print tag, duration_means_by_type[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
