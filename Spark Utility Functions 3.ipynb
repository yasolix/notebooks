{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('/tmp/realEstate.csv')\n",
    "rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "header = rdd.first()\n",
    "rdd = rdd.filter(lambda line:line != header)\n",
    "df = rdd.map(lambda line: Row(street = line[0], city = line[1], zip=line[2], beds=line[4], baths=line[5], sqft=line[6], price=line[9])).toDF()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uberfile = \"/tmp/uber-raw-data-noheader.csv\"\n",
    "uberrdd = sc.textFile(uberfile)\n",
    "def project(rec):\n",
    "    fields = rec.split(',')\n",
    "    return fields\n",
    "    \n",
    "projection = uberrdd.map(lambda rec: project(rec))\n",
    "trans = projection.map(lambda l: [ l[2], l[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "# adding the PySpark modul to SparkContext\n",
    "sc.addPyFile(\"https://raw.githubusercontent.com/seahboonsiew/pyspark-csv/master/pyspark_csv.py\")\n",
    "import pyspark_csv as pycsv\n",
    "collisions = sc.textFile(\"/tmp/NYPD_Motor_Vehicle_Collisions.csv\")\n",
    "\n",
    "collisions_header = collisions.first()\n",
    "collisions_header_list = collisions_header.split(\",\")\n",
    "collisions_body = collisions.filter(lambda line:line != collisions_header)\n",
    "collisions_body = collisions_body.filter(lambda line : len(line.split(\",\"))>29)\n",
    "collisions_df = pycsv.csvToDataFrame(sqlContext, collisions_body, sep=\",\", columns=collisions_header_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "# adding the PySpark modul to SparkContext\n",
    "sc.addPyFile(\"https://raw.githubusercontent.com/seahboonsiew/pyspark-csv/master/pyspark_csv.py\")\n",
    "import pyspark_csv as pycsv\n",
    "collisions = sc.textFile(\"/tmp/NYPD_Motor_Vehicle_Collisions.csv\")\n",
    "def skip_header(idx, iterator):\n",
    "    if (idx == 0):\n",
    "        next(iterator)\n",
    "    return iterator\n",
    "\n",
    "collisions_header = collisions.first()\n",
    "collisions_header_list = collisions_header.split(\",\")\n",
    "collisions_body = collisions.mapPartitionsWithIndex(skip_header)\n",
    "collisions_body = collisions_body.filter(lambda line : len(line.split(\",\"))>29)\n",
    "collisions_df = pycsv.csvToDataFrame(sqlContext, collisions_body, sep=\",\", columns=collisions_header_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_file = \"/tmp/kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "def parse_line(line):\n",
    "    line_split = line.split(\",\")\n",
    "    # keep just numeric and logical values\n",
    "    symbolic_indexes = [1,2,3,41]\n",
    "    clean_line_split = [item for i,item in enumerate(line_split) if i not in symbolic_indexes]\n",
    "    return np.array([float(x) for x in clean_line_split])\n",
    "\n",
    "vector_data = raw_data.map(parse_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_file = \"/tmp/kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "def parse_line_with_key(line):\n",
    "    line_split = line.split(\",\")\n",
    "    # keep just numeric and logical values\n",
    "    symbolic_indexes = [1,2,3,41]\n",
    "    clean_line_split = [item for i,item in enumerate(line_split) if i not in symbolic_indexes]\n",
    "    return (line_split[41], np.array([float(x) for x in clean_line_split]))\n",
    "\n",
    "label_vector_data = raw_data.map(parse_interaction_with_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summary_by_label(raw_data, label):\n",
    "    label_vector_data = raw_data.map(parse_interaction_with_key).filter(lambda x: x[0]==label)\n",
    "    return Statistics.colStats(label_vector_data.values())\n",
    "\n",
    "normal_sum = summary_by_label(raw_data, \"normal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
