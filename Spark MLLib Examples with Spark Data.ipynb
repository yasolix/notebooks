{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.6.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machines (SVMs)\n",
    "\n",
    "The linear SVM is a standard method for large-scale classification tasks. It is a linear method as described above in equation (1), with the loss function in the formulation given by the hinge loss:\n",
    "\n",
    "L(w;x,y):=max{0,1−ywTx}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.38198757764\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "data = sc.textFile(\"file:///home/cloud-user/spark/data/mllib/sample_svm_data.txt\")\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Build the model\n",
    "model = SVMWithSGD.train(parsedData, iterations=100)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))\n",
    "\n",
    "# Save and load model\n",
    "# model.save(sc, \"myModelPath\")\n",
    "# sameModel = SVMModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression is widely used to predict a binary response. It is a linear method as described above in equation (1)\n",
    ", with the loss function in the formulation given by the logistic loss:\n",
    "L(w;x,y):=log(1+exp(−ywTx))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.366459627329\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "data = sc.textFile(\"file:///home/cloud-user/spark/data/mllib/sample_svm_data.txt\")\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(parsedData)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))\n",
    "\n",
    "# Save and load model\n",
    "#model.save(sc, \"/tmp/myModelPath\")\n",
    "#sameModel = LogisticRegressionModel.load(sc, \"/tmp/myModelPath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Linear least squares, Lasso, and ridge regression\n",
    "\n",
    "Linear least squares is the most common formulation for regression problems. It is a linear method as described above in equation (1)\n",
    ", with the loss function in the formulation given by the squared loss:\n",
    "L(w;x,y):=12(wTx−y)2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 7.4510328101\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.replace(',', ' ').split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "data = sc.textFile(\"file:///home/cloud-user/spark/data/mllib/ridge-data/lpsa.data\")\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Build the model\n",
    "model = LinearRegressionWithSGD.train(parsedData, iterations=100, step=0.00000001)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "valuesAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "MSE = valuesAndPreds.map(lambda (v, p): (v - p)**2).reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "print(\"Mean Squared Error = \" + str(MSE))\n",
    "\n",
    "# Save and load model\n",
    "# model.save(sc, \"myModelPath\")\n",
    "# sameModel = LinearRegressionModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
